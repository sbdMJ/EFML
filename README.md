# MPC와 RL 결합을 통한 Adaptive Residual 제어 전략

## 문제 정의: MPC와 RL의 한계 및 결합의 필요성
연속적인 물리 제어 문제에서 MPC와 RL은 각기 장단점을 지니지만, 단독으로 사용될 때 한계가 뚜렷합니다. **모델 예측 제어(MPC)** 는 주어진 시스템 모델을 활용해 향후 동작을 예측하고 최적의 제어 입력을 계산함으로써 높은 안정성과 성능을 보입니다. 그러나 정확한 환경 모델에 대한 의존성이 크고, 모델 불확실성이나 동적 환경 변화에 취약한 한계가 있습니다. 모델이 조금만 부정확해도 MPC 성능은 저하되며, 복잡한 환경에서는 실시간 계산 비용이 높아집니다. <br>

반면 **강화학습(RL)** 은 환경과의 상호작용을 통해 경험으로부터 학습하므로 모델이 정확하지 않아도 적응적으로 최적 정책을 찾을 수 있고, 예측하기 어려운 동적 상황에도 대응할 수 있는 강건성이 장점입니다. 그러나 RL은 초기 탐색 단계의 비효율성과 불안정한 행동으로 인해 학습 과정에서 위험을 초래할 수 있고, 충분한 성능을 내기까지 막대한 데이터가 필요합니다. 실제 로봇이나 공학 시스템에 RL을 바로 적용하면 시행착오 과정에서 안전을 해칠 우려도 있습니다. 이렇듯 RL은 데이터 효율과 안정성 면에서 고전적 제어에 비해 취약한 측면이 있습니다. <br>

이러한 상반된 특성을 고려할 때, MPC와 RL을 조합하여 각각의 강점을 살리고 약점을 보완하는 전략이 주목받고 있습니다. 예컨대 기존 연구들에서도 Residual Reinforcement Learning 기법으로 기본 제어기(예: MPC나 PID 등의 피드백 제어) 위에 RL 에이전트가 잔여 행동(residual action)을 학습하여 성능을 향상시키는 사례들이 보고되었습니다. 실제 로봇 제어에서 기존 제어기+RL 결합은 샘플 효율을 높이고 안전성을 유지하면서 복잡한 과제를 학습하는 데 유용함이 입증되었습니다. <br>

하지만 기존 Residual RL 구조에서는 모든 상태에서 일정한 방식으로 (예컨대 단순 합으로) RL 정책의 출력을 기본 제어기에 더합니다. 다시 말해, MPC와 RL의 결합 비율이 고정되어 있어 환경 상태에 따라 달라지지 않습니다. 이러한 고정 결합은 어떤 상태에서는 비효율적일 수 있습니다. 예를 들어 특정 상태에서는 MPC의 제어만으로도 충분히 최적일 수 있는데, 굳이 RL 잔차 제어를 더하면 노이즈만 추가될 수 있고, 반대로 다른 상태에서는 RL이 더 적극적으로 개입해야 할 필요가 있을 수 있습니다. 상태에 따른 가변적 결합 전략 없이 일률적인 결합을 하면 이러한 미스매치를 해소하기 어렵습니다. <br>

Adaptive Residual 구조는 바로 이 문제의식에서 출발했습니다. 본 프로젝트의 제안된 해결책은 "MPC를 초기 정책으로 사용하되, 상태 $s$에 따라 RL 기반의 잔여 제어 비중 $\lambda(s)$를 가변적으로 조절함으로써 상황에 맞는 최적 결합을 이루는 것입니다. 이를 통해 MPC의 안정성과 RL의 학습 능력을 상태별로 융합하여, 연속 제어 문제에서 두 기법의 한계를 극복하는 것이 목표입니다.

## 기여 내용: PyRocketCraft에서의 Adaptive Residual RL 구현 및 평가
본 프로젝트에서는 위의 Adaptive Residual RL 아이디어를 PyRocketCraft 시뮬레이터 상에서 구현하고 평가함으로써 다음과 같은 기여를 이루었습니다: <br>

+ Adaptive Residual 제어 구조 구현: 오픈소스 로켓 착륙 환경인 PyRocketCraft에서 MPC 기반 제어 정책과 RL 기반 잔차 정책을 통합하는 Adaptive Residual 알고리즘을 설계·구현하였습니다. PyRocketCraft는 PyBullet 물리 엔진을 이용해 3D 로켓 착륙을 시뮬레이션하며, 본래 비선형 MPC(NMPC) 제어기가 내장된 환경입니다. 이 프로젝트에서는 해당 환경에 강화학습 에이전트를 추가하여, MPC의 출력을 보정(correction)하는 RL 잔차 정책을 실행하고 이를 상태에 따라 가중합하도록 시스템을 확장하였습니다.
+ 비교 실험 및 성능 분석: 구현한 Adaptive Residual 기법의 효과를 검증하기 위해 세 가지 제어 전략(① MPC 단독, ② MPC+RL Residual 기본형, ③ MPC+RL Adaptive Residual 제안법)을 동일한 환경에서 비교 실험하였습니다. 각 방법으로 강화학습 훈련을 수행하면서 최종 성능 지표(최종 reward)를 측정하여 분석했습니다. 이를 통해 기존 방식 대비 Adaptive Residual 구조의 장단점을 평가하고, 특히 상태 기반 가중치 $\lambda(s)$의 역할을 모니터링하였습니다.
+ Residual 가중치 동작 검토: Adaptive Residual 기법의 핵심인 적응형 가중치 $\lambda(s)$의 동작 양상을 실험을 통해 관찰하고 해석을 시도했습니다. $\lambda(s)$ 값이 어떻게 분포하는지, 어떤 상태에서 RL 잔차에 더 의존하거나(MPC 대비) 덜 의존하는지 분석함으로써, 제안 기법이 실제로 상태별 결합 변화를 보이는지 그리고 그것이 성능에 미치는 영향을 평가하였습니다.

## 참신성: 상태 기반 Adaptive Residual 결합의 도입
제안하는 Adaptive Residual 구조의 가장 큰 차별점(참신성)은 상태에 따라 달라지는 MPC-RL 결합 비율을 도입했다는 것입니다. 기존의 residual RL 접근법에서는 RL이 기존 제어기의 행동을 일률적으로 보정하는 데 그쳤다면, 본 연구에서는 상황에 따라 보정 정도를 달리할 수 있도록 설계했습니다. 이를 구현하기 위해 상태 의존적 가중치 함수 $\lambda(s)$를 두어, 매 시점 상태 $s$에서 MPC 출력과 RL 잔차 출력을 가중 합하도록 만들었습니다. 최종 제어 입력 $u_{\text{total}}(s)$는 다음과 같이 주어집니다: <br>

$u_{\text{total}}(s)$ = $(1-\lambda(s))u_{\text{MPC}}(s) + \lambda(s)u_{\text{RL}}(s)$ <br>

이 때 $\lambda(s)$는 $0 \le \lambda(s) \le 1$ 범위를 가지는 적응형 가중치로, $\lambda(s)=0$이면 MPC의 결정만 사용하고 $\lambda(s)=1$이면 RL 정책만 따르는 극단을 의미합니다. 대부분의 경우 $\lambda(s)$는 0과 1 사이의 값을 취하면서 MPC와 RL의 비중을 조절하게 됩니다. <br>

이 상태 기반 가중치 결합 아이디어는 안전성 강화 등을 위해 최근 제안된 일부 RL 기법과 맥락을 같이합니다. 예를 들어, Imperial College London 연구진의 RL-AR (Reinforcement Learning with Adaptive Regularization) 알고리즘에서는 **“포커스 모듈”** 을 통해 상태별로 안전 정책(기존 제어기)과 RL 정책을 조합하여, 미탐색 상태에서는 안전한 기존 정책에 더 의존하고 충분히 탐색된 상태에서는 RL 정책의 비중을 높이는 방식을 채택하였습니다. 이러한 접근은 상태에 따라 두 정책의 신뢰도를 동적으로 조절함으로써 안전성과 최적성 두 마리 토끼를 잡는 것을 목표로 합니다. 본 연구의 Adaptive Residual 구조도 이와 유사하게, 상태별로 MPC 대 RL의 신뢰도를 조정하여 필요한 경우에만 RL의 창의성을 끌어내고 불필요할 때는 MPC의 안정성에 의존하도록 합니다. 고정 비율 결합의 단점을 넘어서는 새로운 시도라는 점에서 본 접근법의 의의를 찾을 수 있습니다.

## 구현 및 실험 요약: PyRocketCraft 환경에서의 평가
시뮬레이션 환경 및 구현: 실험은 PyRocketCraft 로켓 착륙 시뮬레이터 환경에서 이루어졌습니다. 이 환경은 OpenAI Gym 인터페이스를 따르는 연속 제어 태스크로, 로켓을 지정된 지점에 연착륙시키는 것이 목표입니다. 기본적으로 PyRocketCraft에는 NMPC (비선형 MPC) 제어기가 구현되어 있어, 로켓의 현재 상태를 받아 최적 추력을 계산해주는 모듈이 있습니다. 이 구조를 확장하여, RL 에이전트가 출력하는 추가적인 보정 힘(residual force)을 MPC의 출력에 더해 최종 제어입력을 생성하도록 했습니다. 적응형 가중치 $\lambda(s)$는 별도의 경량 신경망으로 상태 → [0,1] 값을 출력하도록 설계하였고, 이 값을 통해 MPC 출력과 RL 출력의 가중치를 결정합니다. 요약하면, MPC (전문가 정책)가 기본 제어 신호를 제공하고 RL (학습 정책)이 그 신호를 수정 보완하며, $\lambda(s)$ 신경망이 현재 상태에서 얼마나 RL의 수정을 수용할지를 조절하는 구조입니다.<br>

+ MPC Only: 기존 MPC 제어만으로 로켓을 제어합니다.
+ MPC + RL Residual: 고정 결합의 Residual RL 방식으로, MPC 출력 + RL 잔차를 단순 합합니다. 즉, 모든 상태에 동일한 비율로 RL 잔차를 적용하는 residual 학습 방법입니다 (기존 연구들의 방식).
+ MPC + RL Adaptive Residual: 제안하는 방식으로, 상태마다 $\lambda(s)$를 계산하여 MPC와 RL 출력의 비율을 조절합니다. 상태에 따라서는 MPC 위주(작은 $\lambda$), 다른 상황에서는 RL 위주(큰 $\lambda$)로 제어가 이루어질 수 있습니다.

모든 방법에서 강화학습 알고리즘으로는 PPO 알고리즘을 사용하여, 에피소드 단위로 정책을 학습시켰습니다. RL 정책의 보상 함수는 착륙 오차를 최소화하고 안정적으로 착지하도록 설계되었으며, MPC Only의 경우에는 학습 과정 없이 기존 MPC 모듈을 실행했습니다. MPC+RL Residual과 Adaptive Residual의 초기 단계에서는 MPC 제어가 기본적으로 수행되므로, 에이전트의 초기 탐색이 안정적으로 이루어진다는 장점이 있습니다. 이는 초기부터 탐색하는 순수 RL에 비해 수렴을 빠르게 하고 안전성을 높여줍니다. <br>

성능 비교: 실험 결과, 세 가지 방식 모두 일정 수준의 성능을 달성하였으며 최종적인 임무 성공률이나 보상에서 큰 차이는 나타나지 않았습니다. 아래는 세 접근법의 착륙 결과입니다. <br>

**MPC Only**

https://github.com/user-attachments/assets/fcaca0df-2e1d-487a-965f-1437d1f4f625




<br> **MPC + RL Residual**



https://github.com/user-attachments/assets/725381c9-c8a8-447e-afe6-7d9dbc86fe69





<br> **MPC + RL Adaptive Residual**



https://github.com/user-attachments/assets/b9b0c7d9-d146-436b-82e7-25bf984093e6






## 한계점: 실험 및 방법상의 제약

## 결론 및 향후 연구: Adaptive Residual의 가능성과 개선 방향
